{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<!-- Copyright 2020 InterDigital Communications, Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License. -->"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CompressAI Experiments\n",
    "\n",
    "This notwbook shows some experiments done with the VAE implementation with compressai."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from evaluation_functions import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load a model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from compressai.zoo import bmshj2018_hyperprior\n",
    "\n",
    "inference_ready_model_path = #...\n",
    "\n",
    "net = bmshj2018_hyperprior(quality=5, pretrained=False)\n",
    "net.load_state_dict(torch.load(inference_ready_model_path))\n",
    "net.eval()\n",
    "net.to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in net.parameters())}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"text.usetex\": True,\n",
    "    \"pgf.rcfonts\": False,\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load image and convert to 4D float tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_path = #...\n",
    "\n",
    "original_image = Image.open(image_path).convert(\"RGB\") \n",
    "shape_original = original_image.size\n",
    "shape_input = (256, 256)\n",
    "input_image = original_image.resize(shape_input)  \n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(input_image.resize(shape_original))\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(shape_original)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_tensor = transforms.ToTensor()(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_net = net.forward(input_tensor)\n",
    "output_net[\"x_hat\"].clamp_(0, 1)\n",
    "output_tensor = output_net[\"x_hat\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_image = transforms.ToPILImage()(output_tensor.squeeze().cpu())\n",
    "diff = transforms.ToPILImage()(torch.mean((output_tensor - input_tensor).abs(), axis=1).squeeze().cpu())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "figsize_3_images = (5, 4)\n",
    "matplotlib.rcParams.update({'font.size': 9})\n",
    "\n",
    "from matplotlib import gridspec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "fig, axes = plt.subplots(1, 3, figsize=figsize_3_images)\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "axes[0].imshow(input_image.resize(shape_original))\n",
    "axes[0].title.set_text(\"Original\")\n",
    "\n",
    "axes[1].imshow(output_image.resize(shape_original))\n",
    "axes[1].title.set_text(\"Reconstructed\")\n",
    "\n",
    "axes[2].imshow(diff.resize(shape_original), cmap=\"viridis\")\n",
    "axes[2].title.set_text(\"Difference\")\n",
    "\n",
    "plt.savefig(\"./for_latex/Orig_Recon_Diff_plot.pgf\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let\"s compute some common metrics..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"PSNR: {compute_psnr(input_tensor, output_tensor):.2f}dB\")\n",
    "print(f\"MS-SSIM: {compute_msssim(input_tensor, output_tensor):.4f}\")\n",
    "print(f\"Bit-rate: {compute_bpp(output_net):.3f} bpp\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparison to classical codecs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quality comparison at similar bit-rate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_bpp = compute_bpp(output_net)\n",
    "net_msssim = compute_msssim(input_tensor, output_tensor)\n",
    "rec_jpeg, bpp_jpeg = find_closest_bpp(target_bpp, input_image) \n",
    "rec_jpeg_msssim = compute_msssim(input_tensor, transforms.ToTensor()(rec_jpeg).unsqueeze(0).to(device))\n",
    "gan_image_path = #...\n",
    "gan_image_bpp = #...\n",
    "gan_image = Image.open(gan_image_path)\n",
    "rec_gan_msssim = compute_msssim(input_tensor, transforms.ToTensor()(gan_image.resize(shape_input)).unsqueeze(0).to(device))\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = gridspec.GridSpec(2, 2) #, wspace=0.05, bottom=0.3) \n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "ax.imshow(original_image.resize(shape_original))\n",
    "ax.set_title(\"Original\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(rec_jpeg.resize(shape_original))\n",
    "ax.set_title(f\"Compressed by JPEG ({bpp_jpeg:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[1,0])\n",
    "ax.imshow(output_image.resize(shape_original))\n",
    "ax.set_title(f\"Reconstruction VAE ({target_bpp:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[1,1])\n",
    "ax.imshow(gan_image.resize(shape_original))\n",
    "ax.set_title(f\"Reconstruction GAN ({gan_image_bpp:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.savefig(\"./for_latex/Compare_Compressions_with_JPEG_Same_BPP.pgf\", dpi=400, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "#plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Script for out of domain plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\"\n",
    "matplotlib.rcParams.update({'font.size': 6})\n",
    "def make_prediction(image_path):\n",
    "    original_image = Image.open(image_path).convert(\"RGB\") \n",
    "    shape_original = original_image.size\n",
    "    shape_input = (256, 256)#(384, 192) #(1152,384)\n",
    "    input_image = original_image.resize(shape_input)  \n",
    "    input_tensor = transforms.ToTensor()(input_image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_net = net.forward(input_tensor)\n",
    "    output_net[\"x_hat\"].clamp_(0, 1)\n",
    "    output_tensor = output_net[\"x_hat\"]\n",
    "    output_image = transforms.ToPILImage()(output_tensor.squeeze().cpu())\n",
    "    target_bpp = compute_bpp(output_net)\n",
    "    net_msssim = compute_msssim(input_tensor, output_tensor)\n",
    "    rec_jpeg, bpp_jpeg = find_closest_bpp(target_bpp, input_image)\n",
    "\n",
    "    return (original_image, shape_original, output_image, output_tensor, target_bpp, net_msssim, rec_jpeg, bpp_jpeg)\n",
    "\n",
    "image_path_lion = #..\n",
    "(original_image_lion,\n",
    "    shape_original_lion,\n",
    "    output_image_lion, \n",
    "    output_tensor_lion, \n",
    "    target_bpp_lion, \n",
    "    net_msssim_lion,\n",
    "    rec_jpeg_lion, \n",
    "    bpp_jpeg_lion)  = make_prediction(image_path_lion)\n",
    "gan_image_lion = Image.open(#..) \n",
    "\n",
    "\n",
    "image_path_night = #..\n",
    "(original_image_night,\n",
    "    shape_original_night,\n",
    "    output_image_night, \n",
    "    output_tensor_night, \n",
    "    target_bpp_night, \n",
    "    net_msssim_night,\n",
    "    rec_jpeg_night, \n",
    "    bpp_jpeg_night)  = make_prediction(image_path_night)\n",
    "gan_image_night = Image.open(#..)\n",
    "\n",
    "fig = plt.figure(figsize=(7,2.5))\n",
    "gs = gridspec.GridSpec(2, 4)\n",
    "plt.tight_layout()\n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "ax.imshow(original_image.resize(shape_original_lion))\n",
    "ax.set_title(\"Original corner case\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(rec_jpeg_lion.resize(shape_original_lion))\n",
    "ax.set_title(f\"Compressed by JPEG\\n({bpp_jpeg_lion:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[0,2])\n",
    "ax.imshow(output_image_lion.resize(shape_original_lion))\n",
    "ax.set_title(f\"Reconstruction VAE\\n({target_bpp_lion:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[0,3])\n",
    "ax.imshow(gan_image_lion.resize(shape_original_lion))\n",
    "ax.set_title(f\"Reconstruction GAN\\n({#..:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "ax = plt.subplot(gs[1,0])\n",
    "ax.imshow(original_image_night.resize(shape_original_night))\n",
    "ax.set_title(\"Original night scene\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[1,1])\n",
    "ax.imshow(rec_jpeg_night.resize(shape_original_night))\n",
    "ax.set_title(f\"Compressed by JPEG\\n({bpp_jpeg_night:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[1,2])\n",
    "ax.imshow(output_image_night.resize(shape_original_night))\n",
    "ax.set_title(f\"Reconstruction VAE\\n({target_bpp_night:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[1,3])\n",
    "ax.imshow(gan_image_night.resize(shape_original_night))\n",
    "ax.set_title(f\"Reconstruction GAN\\n({#..:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.savefig(\"./for_latex/compare_cases_compression.pgf\", dpi=400, bbox_inches='tight')\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference Pipeline Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with torch.no_grad():\n",
    "    compress = net.compress(input_tensor)\n",
    "    decompress = net.decompress(compress[\"strings\"], compress[\"shape\"])\n",
    "print(compress.keys())\n",
    "print(decompress.keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_tensor_np = np.asarray(input_image)\n",
    "input_size = input_tensor_np.nbytes / 1024\n",
    "print(f\"Inputs size: {input_size} KBytes\")\n",
    "compressed_np = np.append(np.asarray(compress[\"strings\"]), np.asarray(compress[\"shape\"]))\n",
    "output_size = compressed_np.nbytes / 1024\n",
    "print(f\"Compressed size: {output_size} KBytes\")\n",
    "print(f\"Compression: {round(output_size / input_size * 100, 2)}%\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "decrompress_image = transforms.ToPILImage()(decompress[\"x_hat\"].squeeze().cpu())\n",
    "plt.imshow(decrompress_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Object Detection Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import ImageColor\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageOps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def display_image(image):\n",
    "  fig = plt.figure(figsize=(20, 15))\n",
    "  plt.grid(False)\n",
    "  plt.imshow(image)\n",
    "\n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color,\n",
    "                               font,\n",
    "                               thickness=4,\n",
    "                               display_str_list=()):\n",
    "  \"\"\"Adds a bounding box to an image.\"\"\"\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                ymin * im_height, ymax * im_height)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
    "             (left, top)],\n",
    "            width=thickness,\n",
    "            fill=color)\n",
    "\n",
    "  # If the total height of the display strings added to the top of the bounding\n",
    "  # box exceeds the top of the image, stack the strings below the bounding box\n",
    "  # instead of above.\n",
    "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "  # Each display_str has a top and bottom margin of 0.05x.\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = top + total_display_str_height\n",
    "  # Reverse list and print from bottom to top.\n",
    "  for display_str in display_str_list[::-1]:\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
    "                    (left + text_width, text_bottom)],\n",
    "                   fill=color)\n",
    "    draw.text((left + margin, text_bottom - text_height - margin),\n",
    "              display_str,\n",
    "              fill=\"black\",\n",
    "              font=font)\n",
    "    text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n",
    "  \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
    "  colors = list(ImageColor.colormap.values())\n",
    "\n",
    "  try:\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
    "                              25)\n",
    "  except IOError:\n",
    "    print(\"Font not found, using default font.\")\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "  for i in range(min(boxes.shape[0], max_boxes)):\n",
    "    if scores[i] >= min_score:\n",
    "      ymin, xmin, ymax, xmax = tuple(boxes[i])\n",
    "      display_str = \"{}: {}%\".format(class_names[i].decode(\"ascii\"),\n",
    "                                     int(100 * scores[i]))\n",
    "      color = colors[hash(class_names[i]) % len(colors)]\n",
    "      image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
    "      draw_bounding_box_on_image(\n",
    "          image_pil,\n",
    "          ymin,\n",
    "          xmin,\n",
    "          ymax,\n",
    "          xmax,\n",
    "          color,\n",
    "          font,\n",
    "          display_str_list=[display_str])\n",
    "      np.copyto(image, np.array(image_pil))\n",
    "  return image\n",
    "\n",
    "def run_detector(detector, img):\n",
    "\n",
    "  converted_img  = tf.convert_to_tensor(tf.keras.preprocessing.image.img_to_array(img)[tf.newaxis, ...] / 255.)\n",
    "  result = detector(converted_img)\n",
    "\n",
    "  result = {key:value.numpy() for key,value in result.items()}\n",
    "\n",
    "  print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
    "\n",
    "  image_with_boxes = draw_boxes(\n",
    "      np.array(img), result[\"detection_boxes\"],\n",
    "      result[\"detection_class_entities\"], result[\"detection_scores\"])\n",
    "\n",
    "  display_image(image_with_boxes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
    "detector = hub.load(module_handle).signatures[\"default\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run_detector(detector, input_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run_detector(detector, output_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "converted_img  = tf.convert_to_tensor(tf.keras.preprocessing.image.img_to_array(output_image)[tf.newaxis, ...] / 255.)\n",
    "result = detector(converted_img)\n",
    "result = {key:value.numpy() for key,value in result.items()}\n",
    "result"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}