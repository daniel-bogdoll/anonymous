{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Copyright 2020 InterDigital Communications, Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CompressAI Experiments\n",
    "\n",
    "This notebook shows some experiments done with the VAE implementation with compressai. We test the VAE pipeline, compare outputs with GAN reconstructions and JPEG2000 compression. Further, the object detection pipeline is tested.\n",
    "\n",
    "Author: Jonas Rauch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.evaluation_functions import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a model\n",
    "\n",
    "Inference ready model path must be set here. Also make sure that the quality of the bmshj2018_hyperprior model is set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compressai.zoo import bmshj2018_hyperprior\n",
    "\n",
    "inference_ready_model_path = #...\n",
    "\n",
    "net = bmshj2018_hyperprior(quality=5, pretrained=False)\n",
    "net.load_state_dict(torch.load(inference_ready_model_path))\n",
    "net.eval()\n",
    "net.to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in net.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "# Use matplotlib in pdflatex form.\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"text.usetex\": True,\n",
    "    \"pgf.rcfonts\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load image and convert to 4D float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = #...\n",
    "\n",
    "original_image = Image.open(image_path).convert(\"RGB\") \n",
    "shape_original = original_image.size\n",
    "shape_input = (256, 256)\n",
    "input_image = original_image.resize(shape_input)  \n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(input_image.resize(shape_original))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the network\n",
    "\n",
    "The run is a complete forward pass (encoder and decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = transforms.ToTensor()(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_net = net.forward(input_tensor)\n",
    "output_net[\"x_hat\"].clamp_(0, 1)\n",
    "output_tensor = output_net[\"x_hat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = transforms.ToPILImage()(output_tensor.squeeze().cpu())\n",
    "diff = transforms.ToPILImage()(torch.mean((output_tensor - input_tensor).abs(), axis=1).squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize_3_images = (5, 4)\n",
    "matplotlib.rcParams.update({'font.size': 9})\n",
    "\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare original, reconstruction by plot the difference between those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, axes = plt.subplots(1, 3, figsize=figsize_3_images)\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "axes[0].imshow(input_image.resize(shape_original))\n",
    "axes[0].title.set_text(\"Original\")\n",
    "\n",
    "axes[1].imshow(output_image.resize(shape_original))\n",
    "axes[1].title.set_text(\"Reconstructed\")\n",
    "\n",
    "axes[2].imshow(diff.resize(shape_original), cmap=\"viridis\")\n",
    "axes[2].title.set_text(\"Difference\")\n",
    "\n",
    "plt.savefig(\"./for_latex/Orig_Recon_Diff_plot.pgf\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PSNR: {compute_psnr(input_tensor, output_tensor):.2f}dB\")\n",
    "print(f\"MS-SSIM: {compute_msssim(input_tensor, output_tensor):.4f}\")\n",
    "print(f\"Bit-rate: {compute_bpp(output_net):.3f} bpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to classical codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality comparison at similar bit-rate as shown in paper. \n",
    "Path for GAN created reconstruction and bpp must be input manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bpp = compute_bpp(output_net)\n",
    "net_msssim = compute_msssim(input_tensor, output_tensor)\n",
    "rec_jpeg, bpp_jpeg = find_closest_bpp(target_bpp, input_image) \n",
    "rec_jpeg_msssim = compute_msssim(input_tensor, transforms.ToTensor()(rec_jpeg).unsqueeze(0).to(device))\n",
    "gan_image_path = #...\n",
    "gan_image_bpp = #...\n",
    "gan_image = Image.open(gan_image_path)\n",
    "rec_gan_msssim = compute_msssim(input_tensor, transforms.ToTensor()(gan_image.resize(shape_input)).unsqueeze(0).to(device))\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = gridspec.GridSpec(2, 2) #, wspace=0.05, bottom=0.3) \n",
    "\n",
    "ax = plt.subplot(gs[0,0])\n",
    "ax.imshow(original_image.resize(shape_original))\n",
    "ax.set_title(\"Original\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[0,1])\n",
    "ax.imshow(rec_jpeg.resize(shape_original))\n",
    "ax.set_title(f\"Compressed by JPEG ({bpp_jpeg:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[1,0])\n",
    "ax.imshow(output_image.resize(shape_original))\n",
    "ax.set_title(f\"Reconstruction VAE ({target_bpp:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "ax = plt.subplot(gs[1,1])\n",
    "ax.imshow(gan_image.resize(shape_original))\n",
    "ax.set_title(f\"Reconstruction GAN ({gan_image_bpp:.3f} bpp)\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.savefig(\"./for_latex/Compare_Compressions_with_JPEG_Same_BPP.pgf\", dpi=400, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline Test\n",
    "\n",
    "Test of inference pipeline. Compress and decompress the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    compress = net.compress(input_tensor)\n",
    "    decompress = net.decompress(compress[\"strings\"], compress[\"shape\"])\n",
    "print(compress.keys())\n",
    "print(decompress.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_np = np.asarray(input_image)\n",
    "input_size = input_tensor_np.nbytes / 1024\n",
    "print(f\"Inputs size: {input_size} KBytes\")\n",
    "compressed_np = np.append(np.asarray(compress[\"strings\"]), np.asarray(compress[\"shape\"]))\n",
    "output_size = compressed_np.nbytes / 1024\n",
    "print(f\"Compressed size: {output_size} KBytes\")\n",
    "print(f\"Compression: {round(output_size / input_size * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decrompress_image = transforms.ToPILImage()(decompress[\"x_hat\"].squeeze().cpu())\n",
    "plt.imshow(decrompress_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import ImageColor\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "  \"\"\" Display image.\n",
    "\n",
    "  Args:\n",
    "      image: image to display\n",
    "  \"\"\"\n",
    "  fig = plt.figure(figsize=(20, 15))\n",
    "  plt.grid(False)\n",
    "  plt.imshow(image)\n",
    "\n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color,\n",
    "                               font,\n",
    "                               thickness=4,\n",
    "                               display_str_list=()):\n",
    "  \"\"\"Adds a bounding box to an image.\"\"\"\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                ymin * im_height, ymax * im_height)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
    "             (left, top)],\n",
    "            width=thickness,\n",
    "            fill=color)\n",
    "\n",
    "  # If the total height of the display strings added to the top of the bounding\n",
    "  # box exceeds the top of the image, stack the strings below the bounding box\n",
    "  # instead of above.\n",
    "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "  # Each display_str has a top and bottom margin of 0.05x.\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = top + total_display_str_height\n",
    "  # Reverse list and print from bottom to top.\n",
    "  for display_str in display_str_list[::-1]:\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
    "                    (left + text_width, text_bottom)],\n",
    "                   fill=color)\n",
    "    draw.text((left + margin, text_bottom - text_height - margin),\n",
    "              display_str,\n",
    "              fill=\"black\",\n",
    "              font=font)\n",
    "    text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n",
    "  \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
    "  colors = list(ImageColor.colormap.values())\n",
    "\n",
    "  try:\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
    "                              25)\n",
    "  except IOError:\n",
    "    print(\"Font not found, using default font.\")\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "  for i in range(min(boxes.shape[0], max_boxes)):\n",
    "    if scores[i] >= min_score:\n",
    "      ymin, xmin, ymax, xmax = tuple(boxes[i])\n",
    "      display_str = \"{}: {}%\".format(class_names[i].decode(\"ascii\"),\n",
    "                                     int(100 * scores[i]))\n",
    "      color = colors[hash(class_names[i]) % len(colors)]\n",
    "      image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
    "      draw_bounding_box_on_image(\n",
    "          image_pil,\n",
    "          ymin,\n",
    "          xmin,\n",
    "          ymax,\n",
    "          xmax,\n",
    "          color,\n",
    "          font,\n",
    "          display_str_list=[display_str])\n",
    "      np.copyto(image, np.array(image_pil))\n",
    "  return image\n",
    "\n",
    "def run_detector(detector, img):\n",
    "  \"\"\"Run object detection detector.\n",
    "\n",
    "  Args:\n",
    "      detector: detector model\n",
    "      img: image for object detection\n",
    "  \"\"\"\n",
    "  converted_img  = tf.convert_to_tensor(tf.keras.preprocessing.image.img_to_array(img)[tf.newaxis, ...] / 255.)\n",
    "  result = detector(converted_img)\n",
    "\n",
    "  result = {key:value.numpy() for key,value in result.items()}\n",
    "\n",
    "  print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
    "\n",
    "  image_with_boxes = draw_boxes(\n",
    "      np.array(img), result[\"detection_boxes\"],\n",
    "      result[\"detection_class_entities\"], result[\"detection_scores\"])\n",
    "\n",
    "  display_image(image_with_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
    "detector = hub.load(module_handle).signatures[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_detector(detector, input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_detector(detector, output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_img  = tf.convert_to_tensor(tf.keras.preprocessing.image.img_to_array(output_image)[tf.newaxis, ...] / 255.)\n",
    "result = detector(converted_img)\n",
    "result = {key:value.numpy() for key,value in result.items()}\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
